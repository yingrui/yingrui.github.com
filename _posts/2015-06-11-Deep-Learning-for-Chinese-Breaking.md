---

layout: post
category : nlp
tags : [Chinese Breaking]

---
这篇博客的目的是描述我的想法和一些已经完成的工作，所以尽量不出现任何公式。

##深度学习在中文分词中的应用
现在在机器学习领域，最热的词当属深度学习（Deep Learning）。因为我个人的兴趣在自然语言处理——相比机器学习来说，机器学习就像理科，NLP感觉就像工科——所以我决定要将深度学习应用到中文分词。

###应用Word2Vec表示字与词
深度学习最重要的观点，是Feature Representation，这个不好翻，但是很容易理解。以前，我们都说，要开发一个机器学习应用，最难的部分是特征的选择。针对不同的应用，要设计不同的特征，以及对应的特征提取方法。但是Deep Learning不需要开发人员设计特别的特征，深度学习就是专门学习特征的。

关于特征的问题，就不展开说了，这里重要的是，表示一个字或者词的特征是什么？简单的说，是一个向量。例如：
<ol>
	<li>张： [0.122, -0.3123, ..., 0.0053, ...]</li>
	<li>王： [0.212, -0.0231, ..., 0.0103, ...]</li>
</ol>

但是这个向量怎么来的呢？是基于深度学习的方法自动学习生成的。Word2Vec是一个基于神经网络实现的深度学习库，主要的功能是根据字或词的上下文，自动学习词的特征表示。

我首先简单重写了Word2Vec，考虑到google code已经不复存在了，托管在上面的Word2Vec也很久没人更新了，同时其代码也非常晦涩难懂。于是我重写在我的分词项目中：https://github.com/yingrui/mahjong

这里有广告之嫌，没错，这就是广告:-) Word2Vec是什么具体就没必要在这篇博客中写了，因为参考一手资料一直是我老婆推荐的学习方法。但是有一点，Mikolov提出的一些新的训练方法，都没有在Word2Vec中实现，有兴趣的同学可以继续完善这个库。

###结合Word2Vec实现中文分词

####中文分词de概率模型
使用向量来表示字与词，那怎么实现中文分词呢？首先我们要先来看一个简单的概率模型。
<img src="/assets/20150611/crf-chain-model.png" title="下面一列是字，上面一列是字的标注，Begin表示当前字是一个词的开头，Middle表示当前字是一个词的中间，End表示当前字是词的结尾，Single表示当前字就是一个单字词"/>

这个图描述的是现有的CRF分词的概率模型。下面一列是字，上面一列是字的标注，标注一般有四个：Begin表示当前字是一个词的开头，Middle表示当前字是一个词的中间，End表示当前字是词的结尾，Single表示当前字就是一个单字词。

如果输入字串是：
<ol>
<li>我爱你，那分词结果就是：我/S 爱/S 你/S</li>
<li>北京欢迎你，那分词结果就是：北/B 京/E 欢/B 迎/E 你/S</li>
<li>思特沃克在中国，那分词结果就是：思/B 特/M 沃/M 克/E 在/S 中/B 国/E</li>
</ol>

以“我爱你”为例，在以上图模型里，我们要计算的是当Label1，Label2，Label3分别为什么，以下算式的结果最大，也即概率最大：

P(Label1=? \| Word=我) x P(Label1=? \| Position=居首) x P(Label2=? \| Word=爱) x P(Label2=? \| LastLabel=Label1) x P(Label2=? \| Word=你) x P(Label3=? \| LastLabel=Label2)

我们知道CRF模型可以分别计算P(Label1=? \| Word=我)，P(Label1=? \| Position=居首)。。。然后再使用Vitebi算法就可以算出来我/S 爱/S 你/S是最佳的结果。

####使用Softmax计算概率分布
那么再回过头来看使用深度学习的方法，怎么计算P(Label1=? \| Word=我)呢？一张图读懂Softmax：

<img src="/assets/20150611/softmax.png" title="输入就是通过Word2Vec学习出来的向量，输出就是Label＝?的概率"/>

没错，只要使用经过Word2Vec学习到的向量来表示字，再使用Softmax分类器来学习概率，我们就能计算出P(Label1=? \| Word=我)。然后再按照以上描述的方法，计算出最佳的Label序列是什么？是不是很简单？

####实验结果
我也基本实现了上面描述的方法，因为最近比较忙，没有时间计算大规模的语料，所以只是计算了10万字规模的语料！分类准确率可以接近94%。

这个结果我就只能呵呵了，但是我还有杀手锏没有使出来，等过两天有时间了，再来挑战。到时候更新详细的方法，带公式的，哈哈:-D
